import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re

st.title("ğŸ’¬ YouTube ëŒ“ê¸€ ë¶„ì„ê¸°")

# API í‚¤ (secrets.tomlì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
API_KEY = st.secrets["YOUTUBE_API_KEY"]

# ìœ íŠœë¸Œ ë§í¬ ì…ë ¥
url = st.text_input(
    "YouTube ì˜ìƒ ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
    value="https://www.youtube.com/watch?v=d95J8yzvjbQ"
)

# ì˜ìƒ ID ì¶”ì¶œ í•¨ìˆ˜
def extract_video_id(url):
    patterns = [
        r'v=([a-zA-Z0-9_-]{11})',
        r'youtu\.be/([a-zA-Z0-9_-]{11})',
        r'embed/([a-zA-Z0-9_-]{11})'
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

# ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
@st.cache_data
def get_comments(video_id, max_results=100):
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    
    comments = []
    request = youtube.commentThreads().list(
        part='snippet',
        videoId=video_id,
        maxResults=min(max_results, 100),
        order='relevance'  # ì¸ê¸° ëŒ“ê¸€ ìˆœ
    )
    
    response = request.execute()
    
    for item in response.get('items', []):
        comment = item['snippet']['topLevelComment']['snippet']
        comments.append({
            'author': comment['authorDisplayName'],
            'text': comment['textDisplay'],
            'likes': comment['likeCount'],
            'date': comment['publishedAt'][:10]
        })
    
    return pd.DataFrame(comments)

# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì˜ì–´)
STOPWORDS = {
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
    'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those',
    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'my', 'your', 'his',
    'her', 'its', 'our', 'their', 'me', 'him', 'us', 'them', 'what',
    'which', 'who', 'when', 'where', 'why', 'how', 'all', 'each', 'every',
    'both', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'not',
    'only', 'same', 'so', 'than', 'too', 'very', 'just', 'also', 'now',
    'here', 'there', 'then', 'if', 'about', 'into', 'through', 'during',
    'before', 'after', 'above', 'below', 'from', 'up', 'down', 'out',
    'off', 'over', 'under', 'again', 'further', 'once', 'video', 'like',
    'really', 'much', 'get', 'got', 'im', 'dont', 'cant', 'youre', 'hes',
    'shes', 'its', 'weve', 'theyre', 'ive', 'didnt', 'doesnt', 'wont',
    'br', 'http', 'https', 'www', 'com'
}

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def clean_text(text):
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜
    text = re.sub(r'[^a-zA-Z\s]', '', text).lower()
    # ë¶ˆìš©ì–´ ì œê±°
    words = text.split()
    words = [w for w in words if w not in STOPWORDS and len(w) > 2]
    return ' '.join(words)

# ì‹¤í–‰
if st.button("ëŒ“ê¸€ ë¶„ì„í•˜ê¸°"):
    video_id = extract_video_id(url)
    
    if not video_id:
        st.error("ì˜¬ë°”ë¥¸ YouTube ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            try:
                df = get_comments(video_id)
                
                if df.empty:
                    st.warning("ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # ì¸ê¸° ëŒ“ê¸€ Top 5
                    st.subheader("ğŸ”¥ ì¸ê¸° ëŒ“ê¸€ Top 5")
                    top_comments = df.nlargest(5, 'likes')[['author', 'text', 'likes']]
                    st.dataframe(top_comments, use_container_width=True)
                    
                    # ì›Œë“œí´ë¼ìš°ë“œ
                    st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
                    all_text = ' '.join(df['text'].apply(clean_text))
                    
                    if all_text.strip():
                        wordcloud = WordCloud(
                            width=800,
                            height=400,
                            background_color='white',
                            colormap='viridis',
                            max_words=100
                        ).generate(all_text)
                        
                        fig, ax = plt.subplots(figsize=(10, 5))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                    else:
                        st.info("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                    
                    # í†µê³„
                    st.subheader("ğŸ“Š ëŒ“ê¸€ í†µê³„")
                    col1, col2, col3 = st.columns(3)
                    col1.metric("ì´ ëŒ“ê¸€ ìˆ˜", len(df))
                    col2.metric("ì´ ì¢‹ì•„ìš”", df['likes'].sum())
                    col3.metric("í‰ê·  ì¢‹ì•„ìš”", f"{df['likes'].mean():.1f}")
                    
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì•ˆë‚´
st.divider()
st.markdown("ğŸ”‘ **API í‚¤ ë°œê¸‰ ë°©ë²•**")
st.markdown("[Google Cloud Console](https://console.cloud.google.com/)ì—ì„œ YouTube Data API v3ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
